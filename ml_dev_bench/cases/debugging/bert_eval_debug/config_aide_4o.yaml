# Core evaluation settings
num_runs: 1
fail_fast: false
log_level: "INFO"
output_dir: "./results"
workspace_dir: "./ml_dev_bench/cases/debugging/bert_eval_debug/setup_workspace"
langchain_project: "ml-dev-bench-evaluation-bert-eval-debug"
clone_workspace: true  # clones workspace to a temporary directory

# Task package imports
task_packages:
  - "ml_dev_bench.cases.debugging.bert_eval_debug"

clone_workspace_to: "/workspace"
clear_workspace: true

agent_packages:
  - "agents.aide_ml_agent"

agent:
  id: "aide_ml_agent"
  model_name: "gpt-4o"

# Task configurations
tasks:
  - id: "bert_eval_debug"
