import requests
import json
import io
import zipfile
import torch
from torch.utils.data import Dataset


def load_boolq_datasets(splits, tokenizer, max_length=256):
    """
    Load BoolQ datasets directly from source for given splits and convert them to torch format
    
    Args:
        splits (list): List of splits to load (e.g. ['train', 'validation'])
        tokenizer: HuggingFace tokenizer
        max_length (int): Maximum sequence length for tokenization
        
    Returns:
        list: List of preprocessed torch datasets in the same order as splits
    """
    class BoolQDataset(Dataset):
        def __init__(self, encodings, labels):
            self.encodings = encodings
            self.labels = torch.tensor(labels, dtype=torch.long)

        def __getitem__(self, idx):
            item = {key: val[idx] for key, val in self.encodings.items()}
            item['labels'] = self.labels[idx]
            return item

        def __len__(self):
            return len(self.labels)

    # Download BoolQ data
    url = "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip"
    response = requests.get(url)
    
    datasets = []
    with zipfile.ZipFile(io.BytesIO(response.content)) as z:
        for split in splits:
            # Map 'validation' to 'val' as that's how it's named in the zip
            file_split = 'val' if split == 'validation' else split
            
            # Read the data for the current split
            with z.open(f'BoolQ/{file_split}.jsonl') as f:
                data = [json.loads(line) for line in f.read().decode('utf-8').splitlines()]
            
            # Extract questions, passages, and labels
            questions = [item['question'] for item in data]
            passages = [item['passage'] for item in data]
            labels = [item['label'] for item in data]
            
            # Tokenize the inputs
            tokenized = tokenizer(
                questions,
                passages,
                padding='max_length',
                truncation=True,
                max_length=max_length,
                return_tensors='pt'
            )
            
            # Create dataset
            dataset = BoolQDataset(tokenized, labels)
            datasets.append(dataset)
    
    return datasets if len(datasets) > 1 else datasets[0]
