import json
import time

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.models import mobilenet_v2


def get_transforms():
    # Define transforms for training data
    train_transform = transforms.Compose(
        [
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(15),
            transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,)),
            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),
        ]
    )

    # Define transforms for validation data
    val_transform = transforms.Compose(
        [
            transforms.Resize(32),
            transforms.ToTensor(),
            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),
        ]
    )
    return train_transform, val_transform


def get_datasets():
    train_transform, val_transform = get_transforms()
    # Load CIFAR-10 dataset
    train_dataset = torchvision.datasets.MNIST(
        root='./data', train=True, download=True, transform=train_transform
    )

    val_dataset = torchvision.datasets.MNIST(
        root='./data', train=False, download=True, transform=val_transform
    )
    return train_dataset, val_dataset


def get_dataloaders(iterations_per_epoch):
    # Create data loaders
    train_dataset, val_dataset = get_datasets()
    train_loader = DataLoader(
        train_dataset,
        batch_size=32,
        num_workers=2,
        sampler=torch.utils.data.RandomSampler(
            train_dataset, num_samples=iterations_per_epoch * 32, replacement=True
        ),
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=32,
        num_workers=2,
        sampler=torch.utils.data.RandomSampler(
            val_dataset, num_samples=iterations_per_epoch * 32, replacement=True
        ),
    )
    return train_loader, val_loader


def get_model(num_classes):
    # Initialize the model
    model = mobilenet_v2(pretrained=False)
    # Modify the last layer for CIFAR-10 (10 classes)
    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)

    # Use cpu
    device = 'cpu'
    model = model.to(device)
    return model


def train_one_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(train_loader)
    accuracy = 100.0 * correct / total
    return epoch_loss, accuracy


def validate(model, val_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    val_loss = running_loss / len(val_loader)
    accuracy = 100.0 * correct / total
    return val_loss, accuracy


def run_training(num_epochs=2, iterations_per_epoch=100):
    best_val_acc = 0
    # Set random seed for reproducibility
    torch.manual_seed(42)
    device = 'cpu'

    train_loader, val_loader = get_dataloaders(iterations_per_epoch)
    model = get_model(10)
    # Define loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)
    metrics_history = []  # Add this at the start of run_training

    for epoch in range(num_epochs):
        start_time = time.time()

        # Train
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device
        )

        # Validate
        val_loss, val_acc = validate(model, val_loader, criterion, device)

        # Update learning rate
        scheduler.step(val_loss)

        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')

        epoch_time = time.time() - start_time
        # Replace the print statement with metrics collection
        epoch_metrics = {
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'epoch_time': epoch_time,
        }
        metrics_history.append(epoch_metrics)

        print(
            f'Epoch [{epoch + 1}/{num_epochs}] | '
            f'Train Loss: {train_loss:.4f} | '
            f'Train Acc: {train_acc:.2f}% | '
            f'Val Loss: {val_loss:.4f} | '
            f'Val Acc: {val_acc:.2f}% | '
            f'Time: {epoch_time:.2f}s'
        )

    print(f'Best validation accuracy: {best_val_acc:.2f}%')
    # Add this at the end of run_training
    final_metrics = {'metrics_history': metrics_history, 'best_val_acc': best_val_acc}

    with open('train_metrics.json', 'w') as f:
        json.dump(final_metrics, f, indent=4)
