import argparse
from datetime import datetime

import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import wandb
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import WandbLogger
from torch.utils.data import DataLoader

# Set random seed for reproducibility
pl.seed_everything(42)

# Hyperparameters
BATCH_SIZE = 128
EPOCHS = 50
LEARNING_RATE = 0.001
NUM_CLASSES = 10


# Data augmentation and normalization for training
train_transform = transforms.Compose(
    [
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ]
)

# Only normalization for validation
val_transform = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ]
)


class CIFAR10DataModule(pl.LightningDataModule):
    def __init__(self, batch_size=BATCH_SIZE, num_workers=None):
        super().__init__()
        self.batch_size = batch_size
        # Set num_workers to 0 if on CPU, else 2
        self.num_workers = num_workers if num_workers is not None else 0

    def prepare_data(self):
        torchvision.datasets.CIFAR10(root='./data', train=True, download=True)
        torchvision.datasets.CIFAR10(root='./data', train=False, download=True)

    def setup(self, stage=None):
        self.train_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=True, transform=train_transform, download=True
        )

        self.val_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, transform=val_transform, download=True
        )

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True if torch.cuda.is_available() else False,
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True if torch.cuda.is_available() else False,
        )


class LightningCNN(pl.LightningModule):
    def __init__(self, learning_rate=LEARNING_RATE):
        super().__init__()
        self.save_hyperparameters()

        self.features = nn.Sequential(
            # Layer 1
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            # Layer 2
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            # Layer 3
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            # Layer 4
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            # Layer 5
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
        )

        self.classifier = nn.Linear(256, NUM_CLASSES)
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

    def training_step(self, batch, batch_idx):
        inputs, targets = batch
        outputs = self(inputs)
        loss = self.criterion(outputs, targets)

        _, predicted = outputs.max(1)
        correct = predicted.eq(targets).sum().item()
        accuracy = correct / inputs.size(0) * 100

        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train_acc', accuracy, on_step=True, on_epoch=True, prog_bar=True)

        return loss

    def validation_step(self, batch, batch_idx):
        inputs, targets = batch
        outputs = self(inputs)
        loss = self.criterion(outputs, targets)

        _, predicted = outputs.max(1)
        correct = predicted.eq(targets).sum().item()
        accuracy = correct / inputs.size(0) * 100

        self.log('val_loss', loss, on_epoch=True, prog_bar=True)
        self.log('val_acc', accuracy, on_epoch=True, prog_bar=True)

        return loss

    def configure_optimizers(self):
        return optim.Adam(self.parameters(), lr=self.hparams.learning_rate)


def main():
    # disable cuda by setting cuda visible devices to empty

    # Parse command line arguments
    parser = argparse.ArgumentParser(description='CIFAR10 Training')
    parser.add_argument(
        '--device',
        type=str,
        default='auto',
        choices=['cpu', 'gpu', 'auto'],
        help='Device to run on (default: auto)',
    )
    parser.add_argument(
        '--batch_size',
        type=int,
        default=BATCH_SIZE,
        help='Batch size for training (default: 128)',
    )
    parser.add_argument(
        '--epochs',
        type=int,
        default=EPOCHS,
        help='Number of epochs to train (default: 50)',
    )
    parser.add_argument(
        '--checkpoint',
        type=str,
        default=None,
        help='Path to checkpoint file to resume training from',
    )
    args = parser.parse_args()

    # Create directories for checkpoints
    run_id = datetime.now().strftime('%Y%m%d_%H%M%S')

    # Initialize WandB logger
    wandb_logger = WandbLogger(
        project='cifar10-baseline', name=f'lightning_run_{run_id}'
    )

    # Determine device and number of workers
    if args.device == 'auto':
        accelerator = 'auto'
        num_workers = 2 if torch.cuda.is_available() else 0
    elif args.device == 'cpu':
        accelerator = 'cpu'
        num_workers = 0
    else:  # gpu
        accelerator = 'gpu'
        num_workers = 2

    # Initialize data module and model
    data_module = CIFAR10DataModule(batch_size=args.batch_size, num_workers=num_workers)

    # Load from checkpoint if provided, otherwise create new model
    if args.checkpoint:
        print(f'Loading model from checkpoint: {args.checkpoint}')
        model = LightningCNN.load_from_checkpoint(args.checkpoint)
    else:
        model = LightningCNN()

    # Lightning's checkpoint callback
    lightning_checkpoint_callback = ModelCheckpoint(
        dirpath='lightning_checkpoints',
        filename='cifar10-{epoch:02d}-{val_acc:.2f}',
        save_top_k=1,
        monitor='val_acc',
        mode='max',
    )

    # Initialize trainer with device-specific settings
    trainer = pl.Trainer(
        max_epochs=args.epochs,
        accelerator=accelerator,
        devices=1,
        logger=wandb_logger,
        callbacks=[lightning_checkpoint_callback],
        enable_progress_bar=True,
        log_every_n_steps=50,
    )

    # Train the model
    trainer.fit(model, data_module)

    wandb.finish()


if __name__ == '__main__':
    main()
