"""CIFAR-100 dataset and metrics utilities."""

from typing import Any, Optional

import torch
import torchvision.transforms.functional as F
from torch.utils.data import Dataset
from torchvision.datasets import CIFAR100


class CIFAR100Dataset(Dataset):
    """Dataset class for CIFAR-100 classification."""

    def __init__(
        self,
        split: str,
        transform: Optional[Any] = None,
        image_size: int = 32,
    ):
        """Initialize dataset.

        Args:
            split: Dataset split ('train' or 'test')
            transform: Transform function for data augmentation
            image_size: Target image size for resizing
        """
        is_train = split == 'train'
        self.dataset = CIFAR100(root='./data', train=is_train, download=True)
        self.transform = transform
        self.image_size = image_size

    def __len__(self) -> int:
        return len(self.dataset)

    def __getitem__(self, idx: int) -> dict:
        """Get a single example from the dataset.

        Args:
            idx: Index of the example to get

        Returns:
            dict: Dictionary with image and label tensors
        """
        image, label = self.dataset[idx]

        # Convert PIL image to tensor
        image = F.to_tensor(image)

        # Resize if needed
        if self.image_size != 32:
            image = F.resize(image, [self.image_size, self.image_size], antialias=True)

        # Apply additional transforms if provided
        if self.transform is not None:
            image = self.transform(image)

        return {
            'image': image.contiguous(),
            'labels': torch.tensor(label, dtype=torch.long),
        }


def compute_metrics(predictions: torch.Tensor, true_labels: torch.Tensor) -> dict:
    """Compute classification metrics.

    Args:
        predictions: Predicted class indices (B,)
        true_labels: Ground truth class indices (B,)

    Returns:
        dict: Dictionary containing computed metrics
    """
    # Move tensors to CPU and convert to numpy
    predictions = predictions.detach().cpu().numpy()
    true_labels = true_labels.detach().cpu().numpy()

    # Compute basic metrics
    correct = (predictions == true_labels).sum()
    total = len(true_labels)

    # Return metrics that can be aggregated
    return {
        'total': total,
        'correct': int(correct),
        'class_correct': [int((predictions == i).sum()) for i in range(100)],
        'class_total': [int((true_labels == i).sum()) for i in range(100)],
    }
