PPO implementation based on: https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py