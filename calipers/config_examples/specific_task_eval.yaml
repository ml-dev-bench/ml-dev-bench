# Evaluation configuration for running a set of specified tasks

# Core evaluation settings
num_runs: 1
fail_fast: false
log_level: "INFO"
output_dir: "evaluation/results"
env_file: ".env"  # Default environment file path
langchain_project: "ml-dev-bench-evaluation"

# Agent configuration
agent:
  id: "oh_agent"
  agent_type: "plan_and_execute"
  workspace_dir: "./workspace"
  planner_model: "gpt-4"
  executor_model: "gpt-4"
  replanner_model: "gpt-4"
  runtime_config:
    persistent: true
    environment: {}
    local_config:
      max_tree_items: 2

# Specify packages containing tasks to import
task_packages:
  - ml_workflow_bench.tasks.dataset_download
  - ml_workflow_bench.tasks.dataset_preprocessing
  - custom_tasks.my_task_package


# Task configurations
tasks:
  - id: "dataset_download"
    workspace_dir: "./workspace"
    dataset_url: "https://test-dataset.com/data.zip"
    validation:
      required_files:
        - "data.zip"
        - "dataset/"
      min_size_bytes: 1000

  - id: "dataset_preprocessing"
    workspace_dir: "./workspace"
    input_format: "csv"
    output_format: "parquet"
    validation:
      required_files:
        - "processed_data.parquet"
      expected_columns:
        - "feature_1"
        - "feature_2"
        - "target"

  - id: "random_array_generation"
    workspace_dir: "./workspace"
    validation:
      required_files:
        - "random_arrays.npz"
